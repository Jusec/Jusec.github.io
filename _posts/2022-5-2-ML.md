---
layout:     post
title:      7.机器学习常考内容
subtitle:   机器学习
date:       2022-05-02
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Machine Learning 
    - DeepLearning  
---

### **1.什么是特征归一化**

数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。



**特征归一化的作用：数据归一化后，** 更容易正确的收敛到最优解、提升模型的精度，归一化的另一好处是提高精度、深度学习中数据归一化可以防止模型梯度爆炸

### **2.为什么要用1\*1卷积？**

增加网络的深度（加入非线性）、升维或者是降维、跨通道信息交互（channal 的变换）

### **3.padding的作用**

①保持边界信息，如果没有加padding的话，输入图片最边缘的像素点信息只会被卷积核操作一次，但是图像中间的像素点会被扫描到很多遍，那么就会在一定程度上降低边界信息的参考程度，但是在加入padding之后，在实际处理过程中就会从新的边界进行操作，就从一定程度上解决了这个问题。

②可以利用padding对输入尺寸有差异图片进行补齐，使得输入图片尺寸一致。

③在卷积神经网络的卷积层加入Padding，可以使得卷积层的输入维度和输出维度一致。

④卷积神经网络的池化层加入Padding，一般都是保持边界信息和①所述一样。

### **4. pooling如何反向传播**

Max pooling: 下一层的梯度会原封不动地传到上一层最大值所在位置的神经元，其他位置的梯度为0；

Average pooling: 下一层的梯度会平均地分配到上一层的对应相连区块的所有神经元。



**Pooling的作用和缺点：**

增大感受野、平移不变性、降低优化难度和参数。

缺点:造成梯度稀疏，丢失信息



**感受野的理解**：一个卷积核可以映射原始输入图的区域大小。

**感受野的计算公式？**

![image-20220502180731132](https://s2.loli.net/2022/05/02/KZ8JdXU9LD17kyN.png)

其中lk−1为第k−1层对应的感受野大小，fk为第k层的卷积核大小，或者是池化层的池化尺寸大小。

### **5.反向传播的原理：** 

它的主要思想是由后一级的误差计算前一级的误差，从而极大减少运算量。

### **6.各种数据的channel是指什么意思？**

 每个卷积层中卷积核的数量

### **7.卷积层和全连接层的区别**

全连接层的权重矩阵是固定的，即每一次feature map的输入过来必须都得是一定的大小，所以网络最开始的输入图像尺寸必须固定，才能保证传送到全连接层的feature map的大小跟全连接层的权重矩阵匹配。

卷积层就不需要固定大小了，因为它只是对局部区域进行窗口滑动，所以用卷积层取代全连接层成为了可能。

### **8.网络权重初始化**

把w初始化为0、对w随机初始化、Xavier initialization、He initialization

### **9.讲下Attention的原理**

减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。



**Attention有什么缺点**

Attention模块的参数都是通过label和预测值的loss反向传播进行更新，没有引入其他监督信息，因而其受到的监督有局限，容易对label过拟合。

### **10. AuC，RoC，mAP，Recall，Precision，F1-score**

召回率(Recall) = 预测为真实正例 / 所有真实正例样本的个数。



准确率(Precision) =预测为真实正例 / 所有被预测为正例样本的个数。



Precision：P=TP/(TP+FP) 精准率（查准率），Recall：R=TP/(TP+FN) 召回率（查全率 ）



mAP: mean Average Precision, 即各类别AP的平均值，AP: PR曲线下面积，后文会详细讲解，PR曲线: Precision-Recall曲线。



ROC：全称Receiver Operating Characteristic曲线，常用于评价二分类的优劣。



AUC：全称Area Under Curve，被定义为ROC曲线下的面积，取值范围在0.5到1之间。



F1-score：F1值，又称调和平均数，公式(2)和(3)中反应的precision和recall是相互矛盾的，当recall越大时，预测的覆盖率越高，这样precision就会越小，反之亦然，通常，使用F1-score来调和precision和recall。

![image-20220502181226770](https://s2.loli.net/2022/05/02/kBoUP6vat8lfrOb.png)

### **11. dropout的原理**

**原理：在进行传播的时候删除一些结点，降低网络的复杂性**



**dropout训练和测试有什么区别吗？**

Dropout 在训练时采用，是==为了减少神经元对部分上层神经元的依赖==，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。而在测试时，应该用整个训练好的模型，因此不需要dropout。

**原文：** 在训练过程中，从不同数量的“稀疏”网络中删除样本。在测试时，仅使用权重较小的单个未精简网络，就很容易估算出所有这些精简网络的预测结果的平均值。

### **12.是否了解free anchor**

FreeAnchor基于先进的单级探测器RetinaNet。通过用自由锚帧匹配损失替换RetinaNet的损失。

### **13.** **pytorch** **多gpu训练机制的原理**

Pytorch的多GPU处理接口是==torch.nn.DataParallel(module, device_ids)==，其中module参数是所要执行的模型，而 device_ids 则是指定并行的GPU id列表。



并行处理机制是，首先将模型加载到主GPU上，然后再将模型复制到各个指定的从GPU中，然后将输入数据按batch维度进行划分，具体来说就是每个GPU分配到的数据batch数量是总输入数据的batch除以指定GPU个数。每个GPU将针对各自的输入数据独立进行forward计算，最后将各个GPU的loss进行求和，再用反向传播更新单个GPU上的模型参数，再将更新后的模型参数复制到剩余指定的 GPU 中，这样就完成了一次迭代计算。

<img src="https://s2.loli.net/2022/05/03/chHdiKTRoyZ1jb8.jpg" alt="img" style="zoom:80%;" />

### **14.PyTorch里增加张量维度和减少张量维度的函数**

```python
# 扩大张量：
torch.Tensor.expand(*sizes) → Tensor

# 压缩张量：
torch.squeeze(input, dim=None, out=None) → Tensor
```

### **15.nn.torch.conv2d()的参数**

```python
class torch.nn.Conv2d(in_channels,out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,bias=True)
```

in_channels：输入的通道数目  out_channels：输出的通道数目

kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。

stride：卷积每次滑动的步长为多少，默认是 1

padding：设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding =1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。dilation：控制卷积核之间的间距

### **16. tensorflow搭建网络和训练的流程**

①训练的数据

②定义节点准备接收数据

③定义神经层：隐藏层和预测层

④定义loss表达式

⑤选择optimizer使loss达到最小

### **17.TensorFlow的参数初始化机制**

tf中使用tf.constant_initializer(value)类生成一个初始值为常量value的tensor对象。

tf中使用tf.random_normal_initializer() 类来生成一组符合标准正太分布的tensor。

### **18.TensorFlow** **怎么在网络结构实现一个 if 判断** 布尔类型

### **19.Tensorflow中scope的作用**

在tensorflow中使用tf.name_scope()和tf.variable_scope()函数主要是为了变量共享。

### **20.还有什么办法可以加速python代码吗？**

简要：我补充说可以用GPU、batchsize。然后面试官继续追问还有没有，最后他说了cpu加载数据和gpu训练数据的差异，如果只用cpu加载，那发挥不出gpu的优势，可以==用异步来加速==，即先加载一部分数据到缓存。

### **21.图像的特征提取有哪些算法**

1、SIFT：尺度不变特征变换(Scale-invariant features transform)。SIFT是一种检测局部特征的算法，该算法通过求一幅图中的==特征点==（interest points,or corner points）及其有关==scale==(规模)和==orientation==(方向)的描述子得到特征并进行图像特征点匹配，获得了良好效果。SIFT特征不只具有尺度不变性，即使改变旋转角度，图像亮度或拍摄视角，仍然能够得到好的检测效果



2、SURF:加速稳健特征（Speeded Up Robust Features）。SURF是对SIFT算法的改进，其基本结构、步骤与SIFT相近，但具体实现的过程有所不同。SURF算法的优点是速度远快于SIFT且稳定性好。

> SIFT/SURF为了实现不同图像中相同场景的匹配，主要包括三个步骤：
>
> 1、尺度空间的建立；
>
> 2、特征点的提取；
>
> 3、利用特征点周围邻域的信息生成特征描述子
>
> 4、特征点匹配。

| 比较项目         | **SIFT**                                                     | SURF                                                         |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 尺度空间极值检测 | 使用高斯滤波器，根据不同尺度的高斯差(DOG)图像寻找局部极值    | 使用方形滤波器，利用海森矩阵的行列式值检测极值，并利用积分图加速运算 |
| 关键点定位       | 通过邻近信息插补来定位                                       | 与SIFT类似                                                   |
| 方向定位         | 通过计算关键点局部邻域的方向直方图，寻找直方图中最大值的方向作为关键点的主方向 | 通过计算特征点周围像素点x,y方向的哈尔小波变换，将x、y方向小波变换的和向量的最大值作为特征点方向 |
| 特征描述子       | 是关键点邻域高斯图像梯度方向直方图统计结果的一种表示，是16*8=128维向量 | 是关键点邻域2D离散小波变换响应的一种表示，是16*4=64维向量    |
| 应用中的主要区别 | 通常在搜索正确的特征时更加精确，当然也更加耗时               | 描述子大部分基于强度的差值，计算更快捷                       |

3、HOG:方向梯度直方图（Histogram of Oriented Gradient）。



4、DOG：高斯函数的差分(Difference of Gaussian)



5、LBP特征，Haar**特征等**

### **22.极大似然估计和最大后验估计的区别是什么？**

贝叶斯公式：

![image-20220503160542178](https://s2.loli.net/2022/05/03/bALjpKnveocHNTx.png)

极大似然估计（MLE）：在已经得到试验结果（即样本）的情况下，估计满足这个样本分布的参数，将使这个样本出现的概率最大的那个参数Θ作为真参数Θ的估计。在样本固定的情况下，样本出现的概率与参数Θ之间的函数，称为似然函数。



最大后验概率（MAP）：最大后验估计是根据经验数据，获得对难以观察的量的点估计。与最大似然估计不同的是，最大后验估计融入了被估计量的先验分布，即模型参数本身的概率分布。



最大后验概率估计其实就是多了一个参数的先验概率，也可以认为最大似然估计就是把先验概率认为是一个定值；后验概率 := 似然 * 先验概率

### **23. EM算法---最大期望算法**

在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。



最大期望算法经过两个步骤交替进行计算：

第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

### **24.降维方法**：主成分分析(PCA)、线性判别分析(LDA)、局部线性嵌入(LLE)、拉普拉斯映射(LE)、奇异值分解(SVD)

**PCA原理和执行步骤**：主成分分析(PCA) 是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。是将原空间变换到特征向量空间内，数学表示为AX = γX。

**LDA算法**：LDA是一种有监督的（supervised）线性降维算法。与PCA保持数据信息不同，核心思想：往线性判别超平面的法向量上投影，是的区分度最大（高内聚，低耦合）。LDA是为了使得降维后的数据点尽可能地容易被区分！

### **25.条件随机场**

CRF即条件随机场（Conditional Random Fields），是在给定一组输入随机变量条件下另外一组输出随机变量的条件概率分布模型，它是一种判别式的概率无向图模型，既然是判别式，那就是对条件概率分布建模。

### **26.隐马尔科夫模型（HMM）**

隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。

### **27.伯努利分布**

伯努利分布是指一个分布离散型概率分布

伯努利分布(Bernoulli distribution)又名两点分布或0-1分布。

### **28.余弦相似度距离和欧氏距离的区别？**

**欧式距离**：如果是平面上的两个点 A(x1,y1) 和 B(x2,y2) ，那么 A 与 B 的欧式距离就是

![image-20220503161913478](https://s2.loli.net/2022/05/03/wJi2W1QzI853BSr.png)

**余弦相似度距离：** 余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。

### **29.知道决策树算法吗**？ID3，C4.5和CART树

决策树呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策模型进行分类。



**决策树的分类**：离散性决策树、连续性决策树。

离散性决策树：离散性决策树，其目标变量是离散的，如性别：男或女等；

连续性决策树：连续性决策树，其目标变量是连续的，如工资、价格、年龄等；



> **决策树的优点**：(1)具有可读性，如果给定一个模型，那么过呢据所产生的决策树很容易推理出相应的逻辑表达。
>
> ​			(2)分类速度快，能在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
>
> **决策树的缺点**：（1）对未知的测试数据未必有好的分类、泛化能力，即可能发生过拟合现象，此时可采用剪枝或随机森林。

①ID3 ---- ID3算法最核心的思想是**采用信息增益来选择特征**



②C4.5采用**信息增益比**，用于减少ID3算法的局限（在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的）



③CART算法采用**gini系数**，不仅可以用来分类，也可以解决回归问题

> 三者的差异：
>
> 
>
> 划分标准的差异：
> ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服了 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
>
> 
>
> 使用场景的差异：
> ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；
> ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
>
> 
>
> 样本数据的差异：
> ID3 只能处理离散数据且对缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；
> 从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
>
> 
>
> 样本特征的差异：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
>
> 
>
> 剪枝策略的差异：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正

### **30. K折交叉验证（k-fold cross validation）具体是怎么做的**

K折交叉验证用于模型调优，所有的数据都被用来训练，会导致过拟合==，K折交叉验证可以缓解过拟合==。将数据分为k组，每次从训练集中，抽取出k份中的一份数据作为验证集，剩余数据作为训练集。测试结果采用k组数据的平均值。

### **31.拐点怎么求？**

拐点，又称反曲点，在数学上指改变曲线向上或向下方向的点，直观地说拐点是使切线穿越曲线的点（即连续曲线的凹弧与凸弧的分界点）。

若函数y=f(x)在c点可导，且在点c一侧是凸，另一侧是凹，则称c是函数y=f(x)的拐点。

![图片](https://s2.loli.net/2022/05/03/xKVeHu5i47rzvgE.jpg)

### **32.讲一下偏差和方差**

模型误差 = 偏差(Bias) + 方差(Variance) + 不可避免的误差



偏差：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。



方差：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。

### **33.鞍点的定义：**目标函数在此点上的梯度（一阶导数）值为 0， 但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。

### **34.假设检验的基本思想**

假设检验的基本思想是小概率反证法思想。小概率思想是指小概率事件(P<0．01或P<0．05)在一次试验中基本上不会发生。反证法思想是先提出假设(检验假设Ho)，再用适当的统计方法确定假设成立的可能性大小，如可能性小，则认为假设不成立，若可能性大，则还不能认为假设不成立。

### **35.熵是什么意思，写出熵的计算公式**

熵定义为：信息的数学期望。

![image-20220503164044033](https://s2.loli.net/2022/05/03/KU98ef25RbOTosx.png)

### **37.集成学习（bagging和boosting）bagging和boosting的联系和区别**

Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。



==Boosting==（提升法）：Boosting是一组可将弱学习器提升为强学习器的算法。其工作机制为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。



==Bagging==（套袋法）：Bagging是指采用Bootstrap（有放回的均匀抽样）的方式从训练数据中抽取部分数据训练多个分类器，每个分类器的权重是一致的，然后通过投票的方式取票数最高的分类结果最为最终结果。

> 二者区别：
>
> 1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.Boosting：每一轮的训练集不变(个人觉得这里说的训练集不变是说的总的训练集，对于每个分类器的训练集还是在变化的，毕竟每次都是抽样)，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.
>
> 2）样例权重：Bagging：使用均匀取样，每个样例的权重相等Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.
>
> 3）预测函数：Bagging：所有预测函数的权重相等.Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.
>
> 4）并行计算：Bagging：各个预测函数可以并行生成Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.
>
> 1）Bagging + 决策树 = 随机森林
>
> 2）AdaBoost + 决策树 = 提升树
>
> 3）Gradient Boosting + 决策树 = GBDT

### **38.随机森林的原理**

随机森林属于集成学习（Ensemble Learning）中的bagging算法。在集成学习中，主要分为bagging算法和boosting算法。我们先看看这两种方法的特点和区别。



**Bagging（套袋法）**

bagging的算法过程如下：

从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）

对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）

对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）



**Boosting（提升法）**

boosting的算法过程如下：

对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。

进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）



下面是将决策树与这些算法框架进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT



**随机森林的随机体现在哪里？**

==随机森林的随机性体现在每颗树的训练样本是随机的，树中每个节点的分裂属性集合也是随机选择确定的。==有了这2个随机的保证，随机森林就不会产生过拟合的现象了。



**调参**：一般采用==网格搜索法优化超参数组合==。这里将调参方法简单归纳为三条：1、==分块调参==（不同框架参数分开调参）；2、一次调参不超过三个参数；3、逐步缩小参数范围。

### **39.树模型（RF, GBDT, XGBOOST）**

Adaboost与GBDT两者boosting的不同策略是两者的本质区别。



Adaboost强调Adaptive（自适应），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。

而GBDT则是旨在不断减少残差（回归），通过不断加入新的树旨在在残差减少（负梯度）的方向上建立一个新的模型。——即损失函数是旨在最快速度降低残差。



而XGBoost的boosting策略则与GBDT类似，区别在于GBDT旨在通过不断加入新的树最快速度降低残差，而XGBoost则可以人为定义损失函数（可以是最小平方差、logistic loss function、hinge loss function或者人为定义的loss function），只需要知道该loss function对参数的一阶、二阶导数便可以进行boosting，其进一步增大了模型的泛化能力，其贪婪法寻找添加树的结构以及loss function中的损失函数与正则项等一系列策略也使得XGBoost预测更准确。



XGBoost的具体策略可参考本专栏的XGBoost详述。GBDT每一次的计算是都为了减少上一次的残差，进而在残差减少（负梯度）的方向上建立一个新的模型。



XGBoost则可以自定义一套损失函数，借助泰勒展开（只需知道损失函数的一阶、二阶导数即可求出损失函数）转换为一元二次函数，得到极值点与对应极值即为所求。