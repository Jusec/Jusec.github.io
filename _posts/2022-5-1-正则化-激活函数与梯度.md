---
layout:     post
title:      6.正则化、激活函数与梯度
subtitle:   深度学习
date:       2022-05-01
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Machine Learning 
    - DeepLearning  
---



# 一.正则化

**正则化的原理**：在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性。



机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 l1-norm 和l2-norm，中文称作 L1正则化 和 L2正则化，或者 L1范数 和 L2范数。

> **1. L0、L1、L2正则化**
>
> L0 范数：向量中非0元素的个数。
>
> L1 范数 (Lasso Regularization)：向量中各个元素绝对值的和。
>
> L2 范数(Ridge Regression)：向量中各元素平方和再求平方根。
>

> **2. L1、L2正则化区别，为什么稀疏的解好？**
>
> L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。
>
> 
>
> **实现参数的稀疏有什么好处吗？**
>
> 一个好处是可以简化模型，避免过拟合。另一个好处是参数变少可以使整个模型获得更好的可解释性。

> **3.L1正则化和L2正则化的作用**
>
> L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。
>
> L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。

> **4.正则化有哪几种，分别有什么作用？**
>
> L0 范数和 L1 范数都能够达到使参数稀疏的目的，但 L0 范数更难优化求解，L1 范数是 L0 范数的最优凸近似，而且它比 L0 范数要容易优化求解。
>
> L2 范数不但可以防止过拟合，提高模型的泛化能力，还可以让我们的优化求解变得稳定和快速。L2 范数对大数和 outlier(离群值) 更敏感。
>
> 
>
> **L1、L2范数，L1趋向于0，但L2不会，为什么？**
>
> L1范数更容易产生稀疏的权重，L2范数更容易产生分散的权重

> **5.L1和L2正则先验分别服从什么分布**
>
> L1是拉普拉斯分布，L2是高斯分布。



# **二、激活函数与梯度**

在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。

理想的激活函数应满足两个条件：

1. 输出的分布是零均值的，可以加快训练速度。
2. 激活函数是单侧饱和的，可以更好的收敛。  **单侧饱和能使得神经元对于噪声干扰更具鲁棒性**

### **1.激活函数的意义如下**：

①模拟生物神经元特性，==接受输入后通过一个阈值模拟神经元的激活和兴奋并产生输出==；

②==为神经网络引入非线性，增强神经网络的表达能力==；

③导出神经网络最后的结果(在输出层时)。



**常用的激活函数**？sigmoid，tanh，ReLU, leaky ReLU, PReLU, ELU，random ReLU等。

#### **①sigmoid**

我们通常就用其中最常用的logistic函数来代指sigmoid函数

![image-20220501202702872](https://s2.loli.net/2022/05/01/5MK931hdOvpkU7Y.png)

sigmoid函数和阶跃函数非常相似，但是==解决了光滑和连续的问题，同时它还成功引入了非线性==。由于其值域处在0~1，所以往往被用到二分类任务的输出层做概率预测。



当输入值大于3或者小于-3时，梯度就非常接近0了，在深层网络中，这非常容易造成“梯度消失”（也就是反向传播时误差难以传递到前面一层）而使得网络很难训练。此外，sigmoid函数的均值是0.5，这不符合我们对神经网络内数值期望为0的设想。



> **特点：** 它能够把输入的连续实值变换为0和1之间的输出，特别的是，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
>
> 
>
> **缺点：** 缺点1：在深度神经网络中梯度反向传递时导致==梯度爆炸和梯度消失==，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。
>
> ​	  缺点2：Sigmoid 的 output 不是0均值（即zero-centered）。
>
> ​	  缺点3：其解析式中含有==幂运算==，计算机求解时相对来讲比较耗时。

#### **②tanh函数**

![image-20220501203125422](https://s2.loli.net/2022/05/01/gOUXJQxTKwPZrql.png)

这个tanh函数又被称作双曲正切函数，可以看出它的函数范围是（-1，1）而均值为0，解决了上面sigmoid的一个问题。但是不难发现，该函数依旧没有解决梯度消失的问题。（==解决了sigmoid函数收敛变慢的问题，相对于sigmoid提高了收敛速度。==）

![双曲正切函数图形，来源于网上](https://s2.loli.net/2022/07/05/wLI4YOCrhbKRPp8.png)

#### **③Relu函数**

ReLu函数又叫线性整流单元，应该说是当前最常用的一个激活函数了，尤其是在卷积神经网络和层次较深的神经网络中。

![image-20220501203839391](https://s2.loli.net/2022/05/01/9V3X6kOPdeiNfoT.png)

<img src="https://s2.loli.net/2022/05/01/VLEFgcJGAHCUYsN.png" alt="img" style="zoom: 67%;" />

ReLu将x=0处的光滑曲线替换为了折线，这使得它的计算也相对更加简单，而且有助于随机梯度下降算法收敛（有研究指出收敛加速可达6倍）。当然ReLu函数也能够缓解梯度消失的问题。

> 缺点：1.对于小于0的这部分值，梯度会迅速降为0而不再影响网络训练。这会造成部分神经元从来不被激活，也称作“==死区==”。这也给了ReLu函数的变种很多发挥空间。
>
> ​	2.==梯度更新方向为锯齿路径==. 理想的参数的更新情况可能是：一次更新时，该层一部分w参数增大，而另一部分w参数减小。使用ReLU无法做到。在ReLu中每次更新时，w1/w2 必须同时增大或减小，造成了更新方向的锯齿问题，减缓了训练过程。
>
> ![img](https://s2.loli.net/2022/07/05/1eEbSxtCiR8XmHp.jpg)
>
> 
>
> 优点：1）==解决了gradient vanishing(梯度消失)问题== (在正区间)
>
> ​	 2）计算速度非常快，只需要判断输入是否大于0
>
> ​	 3）收敛速度远快于sigmoid和tanh

#### **④Leaky ReLu**

==LeakyReLU的提出就是为了解决神经元”死亡“问题==

![image-20220501203909307](https://s2.loli.net/2022/05/01/bCqrB1lsLcZxEQa.png)

解决了RELU中的死区问题。

​	==使用LeakyReLU的好处就是：在反向传播过程中，对于LeakyReLU激活函数输入小于零的部分，也可以计算得到梯度(而不是像ReLU一样值为0)，这样就避免了上述梯度方向锯齿问题。==

#### **⑤PReLu**

参数化ReLu（Parameterised ReLu,PReLu）的形式和Leaky ReLu一样，唯一地不同是它将α视作一个可训练的参数而不是人为设定的超参数。这样，就避免了Leaky ReLu中的选定α值的问题。

![image-20220501204020485](https://s2.loli.net/2022/05/01/eOmbQxX4rNgiqHA.png)

#### **⑥andomized Leaky ReLu**

![img](https://s2.loli.net/2022/05/01/vU68pzrRfLls1N9.png)

#### **⑦ELU**

![image-20220501204218077](https://s2.loli.net/2022/05/01/p63ETuF9QG1kib8.png)

​	ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：不会有Deal ReLU问题；输出的均值接近0，zero-centered。

​	它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU

### **2.写出Sigmoid、Sigmoid的导数**

![image-20220501204410696](https://s2.loli.net/2022/05/01/A6VtpeORu2B35hz.png)

### **3. sigmoid和relu的优缺点**

> Relu优点：（1）relu函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象.。而对于sigmod函数，在正负饱和区的梯度都接近于0，可能会导致梯度消失现象。
>
> ​		 （2）Relu函数的导数计算更快，所以使用梯度下降时比Sigmod收敛起来要快很多。
>
> Relu缺点：Relu死亡问题。当 x 是小于 0 的时候，那么从此所以流过这个神经元的梯度将都变成 0；这个时候这个 ReLU 单元在训练中将死亡（也就是参数无法更新），这也导致了数据多样化的丢失（因为数据一旦使得梯度为 0，也就说明这些数据已不起作用）。

> Sigmod优点：具有很好的解释性，将线性函数的组合输出为0，1之间的概率。
>
> Sigmod缺点：（1）激活函数计算量大，反向传播求梯度时，求导涉及除法。
>
> ​	   	（2）反向传播时，在饱和区两边导数容易为0，即容易出现梯度消失的情况，从而无法完成深层网络的训练。

### **4. softmax和sigmoid在多分类任务中的优劣**

多个sigmoid与一个softmax都可以进行多分类.如果==多个类别之间是互斥==的，就应该使用==softmax==，即这个东西只可能是几个类别中的一种。如果==多个类别之间不是互斥==的，使用==多个sigmoid==。

### **5.用softmax做分类函数，假如现在要对1w甚至10w类做分类会出现什么问题？**

过拟合，怎么解决，面试官让自己想(不能使用softmax，使用三元组损失)

​	Triplet loss（三元组损失）的优势在于**细节区分**，即当两个输入相似时，**Triplet loss能够更好地对细节进行建模**，相当于加入了两个输入差异性差异的度量，学习到输入的更好表示，从而在上述两个任务中有出色的表现。

​	Triplet loss优点：

​		基于Triplet loss的神经网络模型可以很好的对**细节进行区分**，Triplet loss通常能在训练中学习到更好的细微的特征feature，更特别的是Triplet loss能够根据模型训练的需要设定一定的**阈值**。

​	Triplet loss缺点：

​		三元组的选取导致数据的分布并不一定均匀，所以在模型训练过程表现**很不稳定，而且收敛慢**，需要根据结果**不断调节参数**，而且Triplet loss比分类损失更容易过拟合。





假如不用交叉熵函数，改用MSE会有什么问题呢？**MSE不会收敛。第一、交叉熵函数计算loss是一个凸优化问题，MSE计算loss输出曲线是波动的，有很多局部极值点，变成一个非凸优化问题，不好收敛；第二、sigmod激活函数和mse一起使用时会出现梯度消失**

### **6.梯度爆炸，梯度消失，梯度弥散是什么，为什么会出现这种情况以及处理办法**

**梯度弥散（梯度消失）**: 通常神经网络所用的激活函数是sigmoid函数，sigmod函数容易引起梯度弥散。这个函数能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))表示两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。



**梯度爆炸**：就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。



梯度消失/爆炸是什么？（反向传播中由于链式求导法则的连乘，如果乘数都比较小趋于0，最终传递到网络输入层的梯度会变得很小（梯度消失），如果乘数都很大，最终的梯度也会变得很大（梯度爆炸），其实二者==都是因为网络太深导致权值更新不稳定，本质上是因为梯度反向传播中的连乘效应）==



**梯度消失与梯度爆炸的产生原因**

梯度消失：（1）隐藏层的层数过多；（2）采用了不合适的激活函数(更容易产生梯度消失，但是也有可能产生梯度爆炸)

梯度爆炸：（1）隐藏层的层数过多；（2）权重的初始化值过大



> **梯度消失与梯度爆炸的解决方案**
>
> （1）用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。
>
> （2）用Batch Normalization。BN层解决梯度消失：随着网络加深，数据整体分布逐渐往激活函数的取值区间的上下限两端靠近（sigmoid的饱和区域），导致反向传播时低层神经网络的梯度消失。而BN把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非饱和区。
>
> （3）LSTM的结构设计也可以改善RNN中的梯度消失问题。
>
> （4）预训练+微调
>
> （5）使用残差网络

