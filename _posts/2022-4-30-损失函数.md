---
layout:     post
title:      损失函数
subtitle:   深度学习
date:       2022-04-30
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - DeepLearning 
---

# 损失函数

## **1.说一下smooth L1 Loss,并阐述使用smooth L1 Loss的优点**

![image-20220430150540750](https://s2.loli.net/2022/04/30/OudSeBU79AbtnTv.png)

> Smooth L1的优点：①相比于L1损失函数，可以收敛得更快。
>
> ​          	 ②相比于L2损失函数，对离群点、异常值不敏感，梯度变化相对更小，训练时不容易跑飞。

<img src="https://s2.loli.net/2022/04/30/vQMBbxiPwUjyn3m.jpg" alt="图片" style="zoom:80%;" />

## **2. L1_loss和L2_loss的区别**

平均绝对误差(L1 Loss): 平均绝对误差（Mean Absolute Error,MAE) 是指模型预测值f(x)和真实值y之间距离的平均值，其公式如下：

![image-20220430150904991](https://s2.loli.net/2022/04/30/1z7NPxhYqAGHrJK.png)

均方误差MSE (L2 Loss):均方误差（Mean Square Error,MSE）是模型预测值f(x) 与真实样本值y 之间差值平方的平均值，其公式如下

![image-20220430150929411](https://s2.loli.net/2022/04/30/w15rgDJE7l2sGQv.png)

## **3.为何分类问题用交叉熵而不用平方损失？啥是交叉熵**

![image-20220430150952462](https://s2.loli.net/2022/04/30/UzKmnGY7yErfb2N.png)

1.用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。

2.使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。



**分类中为什么交叉熵损失函数比均方误差损失函数更常用？**

交叉熵损失函数关于输入权重的梯度表达式与预测值与真实值的误差成正比且不含激活函数的梯度，而均方误差损失函数关于输入权重的梯度表达式中则含有，由于常用的sigmoid/tanh等激活函数存在梯度饱和区，使得MSE对权重的梯度会很小，参数w调整的慢，训练也慢，而交叉熵损失函数则不会出现此问题，其参数w会根据误差调整，训练更快，效果更好。



## **4.一张图片多个类别怎么设计损失函数，多标签分类问题**

多标签分类怎么解决，从损失函数角度考虑

| 分类问题名称 | 输出层使用激活函数 | 对应的损失函数                                   |
| :----------: | :----------------: | ------------------------------------------------ |
|    二分类    |    sigmoid函数     | 二分类交叉熵损失函数（binary_crossentropy）      |
|    多分类    |    Softmax函数     | 多类别交叉熵损失函数（categorical_crossentropy） |
|  多标签分类  |    sigmoid函数     | 二分类交叉熵损失函数（binary_crossentropy）      |

****

(多标签问题与二分类问题关系在上文已经讨论过了，方法是计算一个样本各个标签的损失（输出层采用sigmoid函数），然后取平均值。把一个多标签问题，转化为了在每个标签上的二分类问题。)



## **5. LR的损失函数？它的导数是啥？加了正则化之后它的导数又是啥？**

Logistic regression(逻辑回归)是当前业界比较常用的机器学习方法。



Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题，利用Logistic函数（或称为Sigmoid函数），自变量取值范围为(-INF, INF)，自变量的取值范围为(0,1)，函数形式为：

![image-20220430151631272](https://s2.loli.net/2022/04/30/KxLSVlisf7btEvg.png)

LR的损失函数为交叉熵损失函数。
