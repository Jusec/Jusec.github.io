---
layout:     post
title:      3.损失函数
subtitle:   深度学习
date:       2022-04-30
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - DeepLearning 
---

# 损失函数

## **1.说一下smooth L1 Loss,并阐述使用smooth L1 Loss的优点**

![image-20220430150540750](https://s2.loli.net/2022/04/30/OudSeBU79AbtnTv.png)

> Smooth L1的优点：①相比于L1损失函数，可以收敛得更快。
>
> ​          	 ②相比于L2损失函数，对离群点、异常值不敏感，梯度变化相对更小，训练时不容易跑飞。
>
> 
>
> Smooth L1 Loss 解决L1_loss在零点不平滑问题 
>
> Smooth L1 Loss 比L2_loss的对异常值的鲁棒性更好
>
> 
>
> 使用场景：
>
> ​	1.回归任务
>
> ​	2.特征中有较大数值
>
> ​	3.适合大多数问题（用的最多）

<img src="https://s2.loli.net/2022/04/30/vQMBbxiPwUjyn3m.jpg" alt="图片" style="zoom:80%;" />

## **2. L1_loss和L2_loss的区别**

平均绝对误差(L1 Loss): 平均绝对误差（Mean Absolute Error,MAE) 是指模型预测值f(x)和真实值y之间距离的平均值，其公式如下：

![image-20220430150904991](https://s2.loli.net/2022/04/30/1z7NPxhYqAGHrJK.png)

> 优点：无论对于什么样的输入值，都有着稳定的梯度，不会导致梯度爆炸问题，具有较为稳健性的解
>
> 缺点：在中心点是折点，不能求导，梯度下降时要是恰好学习到w=0就没法接着进行了
>
> 
>
> **L1 loss**在什么场景使用：
>
> ​	1.回归任务
>
> ​	2.简单的模型
>
> ​	3. 由于神经网络通常是解决复杂问题，所以很少使用
>



均方误差MSE (L2 Loss):均方误差（Mean Square Error,MSE）是模型预测值f(x) 与真实样本值y 之间差值平方的平均值，其公式如下

![image-20220430150929411](https://s2.loli.net/2022/04/30/w15rgDJE7l2sGQv.png)

> 优点：各点都连续光滑，方便求导，具有较为稳定的解
>
> 缺点：不是特别的稳健，因为当函数的输入值距离真实值较远的时候，对应loss值很大在两侧，则使用梯度下降法求解的时候梯度很大，可能导致梯度爆炸
>
> 
>
> **L2 loss**在什么场景使用：
>
> ​	1.回归任务
>
> ​	2.数值特征不大
>
> ​	3.问题维度不高

> ==L1_loss、L2 loss、smooth L1 loss三者区别：==
>
> **L1_loss**在零点不平滑，用的较少。一般来说，**l1正则**会制造稀疏的特征，大部分无用的特征的权重会被置为0。
>
> **L2 loss**：对离群点比较敏感，如果feature是unbounded的话，需要好好调整学习率，防止出现梯度爆炸的情况。**l2正则**会让特征的权重不过大，使得特征的权重比较平均。
>
> **smooth L1 loss**结合了L1和L2的优点，修改了零点不平滑问题，且比L2 loss对异常值的鲁棒性更强

## **3.为何分类问题用交叉熵而不用平方损失？啥是交叉熵**

![image-20220430150952462](https://s2.loli.net/2022/04/30/UzKmnGY7yErfb2N.png)

1.用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。

2.使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。



**分类中为什么交叉熵损失函数比均方误差损失函数更常用？**

交叉熵损失函数关于输入权重的梯度表达式与预测值与真实值的误差成正比且不含激活函数的梯度，而均方误差损失函数关于输入权重的梯度表达式中则含有，由于常用的sigmoid/tanh等激活函数存在梯度饱和区，使得MSE对权重的梯度会很小，参数w调整的慢，训练也慢，而交叉熵损失函数则不会出现此问题，其参数w会根据误差调整，训练更快，效果更好。



## **4.一张图片多个类别怎么设计损失函数，多标签分类问题**

多标签分类怎么解决，从损失函数角度考虑

| 分类问题名称 | 输出层使用激活函数 | 对应的损失函数                                   |
| :----------: | :----------------: | ------------------------------------------------ |
|    二分类    |    sigmoid函数     | 二分类交叉熵损失函数（binary_crossentropy）      |
|    多分类    |    Softmax函数     | 多类别交叉熵损失函数（categorical_crossentropy） |
|  多标签分类  |    sigmoid函数     | 二分类交叉熵损失函数（binary_crossentropy）      |

****

(多标签问题与二分类问题关系在上文已经讨论过了，方法是计算一个样本各个标签的损失（输出层采用sigmoid函数），然后取平均值。把一个多标签问题，转化为了在每个标签上的二分类问题。)



## **5. LR的损失函数？它的导数是啥？加了正则化之后它的导数又是啥？**

Logistic regression(逻辑回归)是当前业界比较常用的机器学习方法。



Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题，利用Logistic函数（或称为Sigmoid函数），自变量取值范围为(-INF, INF)，自变量的取值范围为(0,1)，函数形式为：

![image-20220430151631272](https://s2.loli.net/2022/04/30/KxLSVlisf7btEvg.png)

LR的损失函数为交叉熵损失函数。
