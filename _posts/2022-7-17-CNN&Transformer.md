---
layout:     post
title:      17. CNN & Transformer
subtitle:   深度学习
date:       2022-07-17
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Deepling  
---

# CNN



1、卷积层本质上是个特征抽取层，应用在NLP问题中，它捕获到的是单词的n-gram片段信息。

2、CNN存在的问题：

  （1）单卷积层无法捕获远距离特征；可以通过加深网络的层数获得更大的感受野，然而实际情况是CNN做NLP问题就是做不深，做到2到3层卷积层就做不上去了；

  （2）还有一个问题，就是Max Pooling层，无法保持输入句子中单词的位置信息。

​    why？

​       1）RNN因为是线性序列结构，所以很自然它天然就会把位置信息编码进去；

​       2）CNN的卷积核是能保留特征之间的相对位置的，滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了；

​       3）但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了。

  目前的趋势是抛弃Pooling层。

3、CNN的并行度是非常自由也非常高的，这是CNN的一个优点。





# **Visual Transformer 具有如下较好的特性**：

1. **==Long Range 带来的全局特性==**。一方面，**CNN 的 Conv 算子存在感受野较局限的问题**，为扩大网络的关注区域，需多层堆叠卷积-池化结构，但随之带来的问题是 **“有效/真实” 感受野以某个中心为原点向外高斯衰减**，因此 **CNN 通常的有效 Attention 就是图中某一两个较重要的 Parts**。为解决该问题，可设计用于 CNN 的 Attention Module 来得到感受野范围更大而均衡的 Attention Map，其有效性得到了很多工作的证明。另一方面，**Transformer 天然自带的 Long Range 特性 (自注意力带来的全局感受野) 使得从浅层到深层特征图，都较能利用全局的有效信息**，**并且 Multi-Head 机制保证了网络可关注到多个 Discriminative Parts (每个 Head 都是一个独立的 Attention)**，这是 Transformer 与 CNN主要区别之一。
2. **==更好的多模态融合能力==**。一方面，**CNN 擅长解构图像的信息**，因为 **CNN 卷积核卷积运算的本质就是传统数字图像处理中的滤波操作**，但这也使得 **CNN 不擅长融合其他模态的信息**，例如文本、标签、语音、时间等。通常需要用 CNN 提取图像特征 (Feature Embedding)，再用其他模型对其他信息进行 Embedding (如文本的 Token Embedding)，最后在网络末端融合多模态的 Embeddings (**后融合**)。另一方面，Transformer 可在网络的输入端融合多模态信息 (**前融合**)，例如，对于图像，可把对图像通过 Conv 或直接对像素操作得到的初始 Embeddings 馈入 Transformer 中，而 **无需始终保持 H×W×C 的 Feature Map 结构**。类似于 Position Embedding，只要能编码的信息，都可以非常轻松地利用进来。
3. **==Multiple Tasks 能力==**。不少工作证明一个 Transformer 可执行很多任务，因为其 Attention 机制可让网络对不同的 Task 进行不同的学习，一个简单的用法便是加一个 Task ID 的 Embedding。
4. **==更好的表征能力==**。不少工作显示 Transformer 可在多个 CV 任务上取得 SOTA 结果。



# CNN & Transformer

1. ==CNN 是通过不断地堆积卷积层来完成对图像从局部信息到全局信息的提取，不断堆积的卷积层慢慢地扩大了感受野直至覆盖整个图像。==但 ==Transformer== 并不假定从局部信息开始，而是 **==一开始就可以拿到全局信息==，学习难度更大一些，但 ==Transformer 学习长依赖的能力更强==**。另外，从 ViT 的分析来看，**前面层的 “感受野” (论文里是 Mean Attention Distance) 虽迥异但总体较小，后面层的 “感受野“ 越来越大，这说明 ViT 也学习到了和 CNN 相同的范式**。没有 “受限” 的 Transformer 一旦完成好学习，势必会发挥自己这种优势。

2. CNN 对图像问题有天然的 Inductive Bias，如平移不变性等以及 CNN 的仿生学特性，这让 CNN 在图像问题上更容易；相比之下，**Transformer 没有这个天然优势**，**那么学习的难度很大**，往往需要 **更大的数据集 (ViT)** 或 **更强的数据增强 (DeiT)** 来达到较好的训练效果。好在 **Transformer 的迁移效果更好**，大数据集上的 Pretrained 模型可以很好地迁移到小数据集上。还有一个就是 ViT 所说的，==Transformer 的 **Scaling 能力** 很强==，那么进一步 **提升参数量** 或许会带来更好的效果 (就像惊艳的GPT模型)。

3. 目前我们还看到很大一部分工作还是把 Transformer 和现有的 CNN 工作结合在一起，如 ViT 其实也是有 **Hybrid Architecture** (将由 ResNet 提取的特征图馈入 ViT 而非原始图像)。而对于检测和分割这类问题，CNN 方法已经很成熟，难以用 Transformer 一下子完全替换掉。目前的工作大都是 **CNN 和 Transformer 的混合体**，这其中有速度和效果的双重考虑。另外，也要考虑到 **如果输入较大分辨率的图像，Transformer 的计算量会很大，所以 ViT 的输入并不是 Pixel，而是小 Patch**。对于 DETR，其 Transformer Encoder 的输入是 1/32 特征 都有计算量的考虑，不过这肯定是有效果的影响，所以才有后面的改进工作 Deformable DETR。**短期内，CNN 和 Transformer 仍会携手同行**。论文 Rethinking Transformer-based Set Prediction for Object Detection 还是把现有的 CNN 检测模型和 Transformer 思想结合在一起实现了比 DETR 更好的效果 (训练收敛速度也更快)：



# Swin Transformer

1. 之前的ViT中，由于 Self-attention 是全局计算的，所以在图像分辨率较大时不太经济。由于 Locality 一直是视觉建模里非常有效的一种 Inductive Bias，所以我们将图片切分为 **无重合的 Windows**，然后在 Local Window 内进行 Self-attention 的计算。为了让 Windows 之间有信息交换，我们在相邻两层使用不同的 Windows 划分 (Shifted Window)。

2. 图片中的物体大小不一，而 ViT 中使用固定的 Scale 进行建模或许对下游任务例如目标检测而言不是最优的。在这里我们还是 Follow 传统 CNN 构建了一个 **层次化的 Transformer 模型**，从 4x 逐渐降分辨率到 32x，这样也可以在任意框架中无缝替代之前的 CNN 模型。

3. MSA：FLOPS:
	$$
	\Omega(M S A)=4 h w C^{2}+2(h w)^{2} C
	$$
	W-MSA & SW-MSA: FLOPS：
	$$
	\Omega(W-M S A)=4 h w C^{2}+2 M^{2} h w C=\Omega(S W-M S A)
	$$
	==W-MSA目的==：减少很大计算量，尤其是在浅层网络

	缺点:窗口间无法进行信息交互

	

	==SW-MSA==:进行窗口间的信息交互

	

	==Relative Position Bias==(相对位置偏置):对模型性能有明显提升
	
	

# CNN & Transformer

#### CNN概念：

传统意义上的多层神经网络是只有输入层、隐藏层、输出层，其中隐藏层的层数按需决定。而卷积神经网络CNN，在传统的多层神经网络基础上，全连接层前面加入了部分连接的卷积层、激活层和池化层操作，使得实际应用场景中能够构建更加深层、功能更强大的网络。

CNN优点：

CNN主要用于识别位移、缩放及其它扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以能够避免显示的特征抽取，隐式地从训练数据中进行学习。

（1）使用局部感知,能够很好的提取局部信息

（2）CNN的权重共享的结构可以使得网络并行学习，较好的处理高维数据，更接近于实际的生物神经网络，

（3）不需要手动进行特征选择,只要训练好卷积核W和偏置项b, 即可得到特征值.

CNN缺点：

（1）当网络层数太深时，采用反向传播调整内部参数会使得接近于输入层的变化较慢；

（1）采用梯度下降进行迭代时很容易使得训练结果收敛于局部最优而非全局最优；

（3）池化层会丢失一定的有价值信息，忽略了局部与整体之间的关联性；

（4）特征提取的物理含义不是十分明确，导致可解释性一般。



#### RNN优点：

传统网络无法结合上下文去训练模型，导致对于序列特性数据场景的处理效果不佳，而RNN结构决定了其具有了短期记忆性，每一时刻隐藏层信息不仅由该时刻的输入层决定，还可以由上一时刻的隐藏层决定，做到挖掘数据的时序信息以及语义信息。

RNN缺点：

随着网络层数增加，RNN在长序列场景处理时会出现梯度消失或梯度爆炸的弊端（反向传播算法局限性导致）。



#### LSTM优点：

LSTM通过引入包含了遗忘门、输入门、输出门的cell状态的结构改善了RNN中存在的长期依赖问题，并且其表现通常比时间递归神经网络和隐马尔科夫模型更好，而LSTM本身也可以作为复杂的非线性单元构造更大型深度网络。

LSTM缺点：

梯度问题在LSTM中得到了一定程度的优化解决，但是并没有彻底搞定，在处理N量级的序列有一定效果，但是处理10N或者更长的序列依然会暴露，另外，**每一个LSTM的单元节点都意味着有4个全连接层，如果时间序列跨度较大，并且网络较深，会出现计算量大和耗时偏多的问题。**



#### Transformer优点：

（1）突破了RNN模型不能并行计算的限制；

（2）相比CNN，计算两个位置之间的关联所需要的操作次数不会随着距离的增长而增加；

（3）attention机制可以产生更具==可解释性的模型==，可以从模型中检查attention分布，各个attention head可以学会执行不同的任务。

Transformer缺点：

（1）局部信息的获取不如RNN和CNN强；

（2）位置信息编码存在问题，因为位置编码在语义空间中并不具备词向量的可线性变换，只是相当于人为设计的一种索引，所以并不能很好表征位置信息；

（3）由于transformer模型实际上是由残差模块和层归一化模块组合而成，并且层归一化模块位于两个残差模块之间，导致如果层数较多时连乘计算会使得顶层出现梯度消失问题。