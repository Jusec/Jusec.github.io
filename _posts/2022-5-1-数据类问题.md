---
layout:     post
title:      4.数据类问题
subtitle:   深度学习
date:       2022-05-01
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - DeepLearning  
---



# 数据类问题

## **1.样本不平衡的处理方法**

> ①==欠采样== - 随机删除观测数量足够多的类，使得两个类别间的相对比例是显著的。虽然这种方法使用起来非常简单，但很有可能被我们删除了的数据包含着预测类的重要信息。**该方法适用于数据量较大的情况。**
>
> 
>
> ②==过采样== - 对于不平衡的类别，我们使用拷贝现有样本的方法随机增加观测数量。理想情况下这种方法给了我们足够的样本数，但过采样可能导致**==过拟合==**训练数据。**该方法适用于数据量较小的情况。**
>
> 
>
> ③==合成采样==（ SMOTE ）-该技术要求我们用**合成方法**得到不平衡类别的观测，该技术与现有的使用最近邻分类方法很类似。SMOTE算法是一种过采样的算法。这个算法不是简单的复制已有的数据，而是在原有数据基础上，通过算法产生新生数据。
>
> 算法思想：基于距离度量的方式计算两个或多个稀有类样本之间的相似性。
>
> 　　　　　　　然后选择其中的一个样本作为基础样本，
>
> 　　　　　　　再在邻居样本中随机选取一定数量的样本对那个基础样本的一个属性进行噪声。每次处理一个属性，通过这样的方式产生新生数据。
>
> 问题在于当一个类别的观测数量极度稀少时该怎么做。比如说，我们想用图片分类问题确定一个稀有物种，但我们可能只有一幅这个稀有物种的图片。
>
> 
>
> ④在loss方面，采用==focal loss==等loss进行控制不平衡样本。
>
> 
>
> ⑤**==使用多个分类器进行分类==。**有以下两种方法可以解决欠采样所带来的问题。
>
> ​	方法一：模型融合 （bagging的思想 ）
>
> 　　思路：从丰富类样本中随机的选取（有放回的选取）和稀有类等量样本的数据。和稀有类样本组合成新的训练集。这样我们就产生了多个训练集，并且是互相独立的，然后训练得到多个分类器。
>
> 　　　　　若是分类问题，就把多个分类器投票的结果（少数服从多数）作为分类结果。
>
> 　　　　　若是回归问题，就将均值作为最后结果。
>
> 　　方法二：增量模型 （boosting的思想）
>
> 　　思路：使用全部的样本作为训练集，得到分类器L1
>
> 　　　　　从L1正确分类的样本中和错误分类的样本中各抽取50%的数据，即循环的一边采样一个。此时训练样本是平衡的。训练得到的分类器作为L2.
>
> 　　　　　从L1和L2分类结果中，选取结果不一致的样本作为训练集得到分类器L3.
>
> 　　　　　最后投票L1,L2,L3结果得到最后的分类结果。
>
> 
>
> ⑥**==改变正负类别样本在模型中的权重。==**
>
> 　　使用代价函数学习得到每个类的权值，大类的权值小，小类的权值大。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。
>
> 
>
> **不平衡类别会造成问题有两个主要原因**：
>
> 1.对于不平衡类别，我们不能得到实时的最优结果，因为模型/算法从来没有充分地考察隐含类。
> 2.它对验证和测试样本的获取造成了一个问题，因为在一些类观测极少的情况下，很难在类中有代表性。



## **2.讲下数据增强有哪些方法**

数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生**等价于更多数据**的价值。

==数据增强目的==：
1. 增加数据集中相关数据的数据量，

   			 2. 减少数据集中不相关的特征
   			 3. 增加训练的数据量，提高模型的泛化能力
   			 4. 增加噪声数据，提升模型的鲁棒性



数据增强可以分为，**有监督**的数据增强和**无监督**的数据增强方法。

1. 其中有监督的数据增强又可以分为**单样本**数据增强和**多样本**数据增强方法。
2. 无监督的数据增强分为**生成新的数据**和**学习增强策略**两个方向。



**==有监督的数据增强==**：

​	有监督数据增强，即采用**预设的数据变换规则**，在已有数据的基础上进行数据的扩增。
​	它包含**单样本**数据增强和**多样本**数据增强。
​	其中**单样本**又包括**几何操作类**，**颜色变换**类。



**空间几何变换**：裁剪，翻转，旋转，缩放，仿射变换，视觉变换（四点透视变换）



**像素颜色变换**：噪声(**添加适量的噪音可以增强学习能力**),模糊，HSV对比度变换，RGB颜色扰动，随机擦除，超像素法，转换，边界检测，锐化与浮雕



**多样本合成类**：(1)SMOTE（Synthetic Minority Over-sampling Technique），通过人工合成新样本来处理样本不平衡问题，提升分类器性能。
 		  (2)SamplePairing方法，从训练集中随机抽取两张图片分别经过基础数据增强操作（如随机翻转等）处理后经像素,以平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。
 		  (3)mixup是基于邻域风险最小化（VRM）原则的数据增强方法，使用线性插值得到新样本数据。

![image-20220501212520852](https://s2.loli.net/2022/05/01/sy5N2hL1dFUqAQw.png)

总结：mixup、SMOTE、SamplePairing三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布，但所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。



==无监督的数据增强:==

无监督的数据增强方法包括**两类**：

1. 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN。

2. 通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment

	

AutoAugment是Google提出的**自动选择最优数据增强方案**的研究，这是无监督数据增强的重要研究方向

它的基本思路是使用**增强学习**从数据本身**寻找最佳图像变换策略**，对于不同的任务学习不同的增强方法，

**解决图像细节不足问题**（增强特征提取骨干网络的表达能力）



## **3.过拟合的解决办法**

数据扩充/数据增强/更换小网络/正则化/dropout/batch normalization



增加训练数据、减小模型复杂度、正则化,L1/L2正则化、集成学习、早期停止



增加噪声、权值共享、剪枝处理



**什么是过拟合**

过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。



**产生过拟合根本原因:**

观察值与真实值存在偏差, 训练数据不足，数据太少，导致无法描述问题的真实分布, 数据有噪声, 训练模型过度，导致模型非常复杂



**什么是欠拟合**：训练的模型在训练集上面的表现很差，在验证集上面的表现也很差

**原因**：训练的模型太简单，最通用的特征模型都没有学习到

