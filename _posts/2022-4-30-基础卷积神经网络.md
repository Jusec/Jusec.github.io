---
layout:     post
title:      2.基础卷积神经网络
subtitle:   深度学习
date:       2022-04-30
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - DeepLearning
---

# 基础卷积神经网络

##  **1.CNN的经典模型**

LeNet，AlexNet，VGG，GoogLeNet，ResNet，DenseNet



## **2.对CNN的理解**

CNN＝ 数据输入层 (Input Layer)＋ {[卷积计算层（CONV Layer )＊ａ＋ReLU激励层 (ReLU Layer)]＊ｂ＋ 池化层 (Pooling Layer ) ｝*ｃ＋全连接层 (FC Layer) ＊ ｄ 。



## **3.CNN和传统的全连接神经网络有什么区别？**

在全连接神经网络中，每相邻两层之间的节点都有边相连，于是会将每一层的全连接层中的节点组织成一列，这样方便显示连接结构。而对于卷积神经网络，相邻两层之间只有部分节点相连，为了展示每一层神经元的维度，一般会将每一层卷积层的节点组织成一个三维矩阵。全连接神经网络和卷积神经网络的唯一区别就是==神经网络相邻两层的连接方式==。



## **4.讲一下CNN，每个层及作用**

卷积层：**用它来进行特征提取**

池化层：对输入的特征图进行压缩，一方面**使特征图变小，简化网络计算复杂度**；一方面**进行特征压缩，提取主要特征**，

激活函数：是用来**加入非线性因素的，因为线性模型的表达能力不够**。

全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。全连接层则起**到将学到的“分布式特征表示”映射到样本标记空间的作用**。



## **5.为什么神经网络使用卷积层**？-共享参数，局部连接；

**使用卷积层的前提条件是什么**？-数据分布一致



## **6.resnet相比于之前的卷积神经网络模型中，最大的改进点是什么？，解决了什么问题**

==跳跃连接(residual block)和瓶颈层==。resnet本身是一种拟合残差的结果，让网络学习任务更简单，可以有效地解决==梯度弥散==的问题。



**Resnet为啥能解决梯度消失，怎么做的，能推导吗？**

由于每做一次卷积（包括对应的激活操作）都会浪费掉一些信息：比如卷积核参数的随机性（盲目性）、激活函数的抑制作用等等。这时，ResNet中的shortcut相当于把以前处理过的信息直接再拿到现在一并处理，起到了减损的效果。



## **7.resnet第二个版本做了哪些改进，Resnet性能最好的变体是哪个，结构是怎么样的，原理是什么？**

<img src="https://s2.loli.net/2022/04/30/ta9cOqBEIsnkLVX.jpg" alt="图片" style="zoom:50%;" />

> Resnetv2：1、相比于原始的网络结构，先激活的网络中f是恒等变换，这使得模型优化更加容易。
>          2、使用了先激活输入的网络，能够==减少网络过拟合==。

**Resnet性能最好的变体是Resnext。**

![图片](https://s2.loli.net/2022/04/30/jOuzKy1eHU2xkcp.jpg)

ResNeXt可以说是基于Resnet与Inception中的'Split + Transfrom + Concat'而搞出的产物，结构简单、易懂又足够强大。（Inception网络使用了一种split-transform-merge思想，即先将输入切分到不同的低维度中，然后做一个特征映射，最后将结果融合到一起。但==模型的泛化性不好==，针对不同的任务需要设计的东西太多。）



ResNeXt提出了一个基数（cardinatity）的概念，用于作为模型复杂度的另外一个度量。**基数（cardinatity）指的是一个block中所具有的相同分支的数目。**



与 ResNet 相比，相同的参数个数，结果更好：一个 101 层的 ResNeXt 网络，和 200 层的 ResNet 准确度差不多，但是计算量只有后者的一半。



> **ResNet的特点:引入跳跃连接，有效地解决了网络过深时候梯度消失的问题，使得设计更深层次的网络变得可行**。

## **8.简述InceptionV1到V4的网络、区别、改进**

InceptionV1的核心就是把Googlenet的某一些大的卷积层换成(1×1),(3×3),(5×5)的==小卷积==，这样能够大大的==减小权值参数数量==。



InceptionV2在输入的时候==增加了batch_normal==，所以他的论文名字也是叫batch_normal，加了这个以后==训练起来收敛更快，学习起来自然更高效，可以减少dropout的使用==。



InceptionV3把Googlenet里一些==(7×7)的卷积变成了(1×7)和(7×1)的两层串联==，(3×3)的也一样，变成了(1×3)和(3×1)，这样==加速了计算，还增加了网络的非线性，减小过拟合的概率。==另外，网络的输入从224改成了299.



InceptionV4实际上是把原来的Inception==加上了Resnet的方法==，从一个节点能够跳过一些节点直接连入之后的一些节点，并且残差也跟着过去一个。另外就是V4把一个先(1×1)再(3×3)那步换成了先(3×3)再(1×1).

论文说引入resnet不是用来提高深度，进而提高准确度的，只是用来提高速度的。



## **9. DenseNet为什么比ResNet有更强的表达能力？**

DenseNet在增加深度的同时，加宽每一个DenseBlock的网络宽度，能够增加网络识别特征的能力，而且由于DenseBlock的横向结构类似 Inception block的结构，使得需要计算的参数量大大降低。

> DenseNet作为另一种拥有较深层数的卷积神经网络,具有如下优点:
>
> (1) 相比ResNet拥有更少的参数数量.
>
> (2) 旁路加强了特征的重用.
>
> (3) 网络更易于训练,并具有一定的正则效果.
>
> (4) 缓解了gradient vanishing和model degradation的问题.

