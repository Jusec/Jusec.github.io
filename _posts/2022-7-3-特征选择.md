```
layout:     post
title:      15.特征选择
subtitle:   深度学习
date:       2022-07-03
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Deepling 
```

# 1.特征选择

**特征选择**：是在原始数据中选取有效特征以降低数据[维度](https://so.csdn.net/so/search?q=维度&spm=1001.2101.3001.7020)，提高模型性能。

目的：

- 1.简化模型，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握

- 2.改善性能：节省存储和计算开销

- 3.改善通用性、降低过拟合风险：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差

	

	 **特征选择的4个步骤**

- 1.产生过程：产生特征或特征子集候选集合

- 2.评价函数：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏

- 3.停止准则：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止

- 4.验证过程：在验证数据集上验证选出来的特征子集的有效性

	

 **特征选择的三个方法：Filter(过滤法)、Wrapper(包装法)、Embedded(嵌入法)**



### 1.1 Filter(过滤法)

​	按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选，分为单变量过滤方法和多变量过滤方法

- **单变量过滤方法**：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。**优点是计算效率高、不易过拟合**
- **多变量过滤方法**：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择

> 优点：
>
> ​	不依赖于任何机器学习方法，且不需要交叉验证，计算效率比较高
>
> 缺点：
>
> ​	没有考虑机器学习算法的特点

![img](https://s2.loli.net/2022/07/03/kvGi5BTp6Eq4Woj.jpg) 



**常用的过滤方法：**

- **覆盖率**

- **方差选择法**

- **Pearson(皮尔森)相关系数**

- **卡尔检验**

- **互信息法(KL散度、相对熵)和最大信息系数 Mutual information and maximal information coefficient (MIC)**

- **Fisher得分**

- **相关特征选择(Correlation Feature Selection, CFS)**

- **最小冗余最大相关性(Minimum Redundancy Maximum Relevance, mRMR)**
