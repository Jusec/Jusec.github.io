---
layout:     post
title:      13.手撕代码
subtitle:   深度学习
date:       2022-05-30
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Deepling 
---

# **1.计算卷积网络输出尺寸**

卷积神经网络的计算公式为：$N=\frac{(W-F+2 P)}{S}+1$

其中N：输出大小

W：输入大小 F：卷积核大小 P：填充值的大小 S：步长大小

在Pytorch中：torch.nn.Conv2d()

![image-20220530200651628](https://s2.loli.net/2022/05/30/rstZJ1NkFlK6bXq.png)

![image-20220530200749022](https://s2.loli.net/2022/05/30/QkWTr2HjuXc7gzK.png)

torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)

![image-20220530200809073](https://s2.loli.net/2022/05/30/rQFO8RmalsM1WhT.png)

#### 参数dilation——扩张卷积（也叫[空洞卷积](https://so.csdn.net/so/search?q=空洞卷积&spm=1001.2101.3001.7020)）

dilation操作动图演示如下：
Dilated Convolution with a 3 x 3 kernel and dilation rate 2
扩张卷积核为3×3，扩张率为2

![在这里插入图片描述](https://s2.loli.net/2022/05/30/U1gSOaDeyJdIvFs.webp)

#### 参数groups——分组卷积
![image-20220530200902627](https://s2.loli.net/2022/05/30/3jn6kOzpH5gafeT.png)

![在这里插入图片描述](https://s2.loli.net/2022/05/30/wAtorb4syICHWOh.png)



# **2. NMS**

```python
import numpy as np
def py_cpu_nms(dets, thresh):
    """Pure Python NMS baseline."""
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]  #[::-1]表示降序排序，输出为其对应序号
    keep = []                     #需要保留的bounding box
    while order.size > 0:
        i = order[0]              #取置信度最大的（即第一个）框
        keep.append(i)            #将其作为保留的框
        #以下计算置信度最大的框（order[0]）与其它所有的框（order[1:]，即第二到最后一个）框的IOU，以下都是以向量形式表示和计算
        xx1 = np.maximum(x1[i], x1[order[1:]]) #计算xmin的max,即overlap的xmin
        yy1 = np.maximum(y1[i], y1[order[1:]]) #计算ymin的max,即overlap的ymin
        xx2 = np.minimum(x2[i], x2[order[1:]]) #计算xmax的min,即overlap的xmax
        yy2 = np.minimum(y2[i], y2[order[1:]]) #计算ymax的min,即overlap的ymax
        w = np.maximum(0.0, xx2 - xx1 + 1)      #计算overlap的width
        h = np.maximum(0.0, yy2 - yy1 + 1)      #计算overlap的hight
        inter = w * h                           #计算overlap的面积
        ovr = inter / (areas[i] + areas[order[1:]] - inter) #计算并，-inter是因为交集部分加了两次。
        inds = np.where(ovr <= thresh)[0]#本轮，order仅保留IOU不大于阈值的下标
        order = order[inds + 1]                    #删除IOU大于阈值的框
return keep
```



# **3. 手写计算IOU代码**

```python
def IOU(x1,y1,X1,Y1, x2,y2,X2,Y2):
    xx = max(x1,x2)
    XX = min(X1,X2)
    yy = max(y1,y2)
    YY = min(Y1,Y2)
    m = max(0., XX-xx)
    n = max(0., YY-yy)
    Jiao = m*n
    Bing = (X1-x1)*(Y1-y1)+(X2-x2)*(Y2-y2)-Jiao
return Jiao/Bing

def bb_intersection_over_union(boxA, boxB):
    boxA = [int(x) for x in boxA]
    boxB = [int(x) for x in boxB]
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
    iou = interArea / float(boxAArea + boxBArea - interArea)
return iou
```



# **4. 手撕SoftNMS**

```python
import numpy as np
def soft_nms(dets, sigma=0.5, Nt=0.5, method=2, threshold=0.1):
    box_len = len(dets)   # box的个数
    for i in range(box_len):
        tmpx1, tmpy1, tmpx2, tmpy2, ts = dets[i, 0], dets[i, 1], dets[i, 2], dets[i, 3], dets[i, 4]
        max_pos = i
        max_scores = ts
        # get max box
        pos = i+1
        while pos < box_len:
            if max_scores < dets[pos, 4]:
                max_scores = dets[pos, 4]
                max_pos = pos
            pos += 1
        # add max box as a detection
        dets[i, :] = dets[max_pos, :]
        # swap ith box with position of max box
        dets[max_pos, 0] = tmpx1
        dets[max_pos, 1] = tmpy1
        dets[max_pos, 2] = tmpx2
        dets[max_pos, 3] = tmpy2
        dets[max_pos, 4] = ts
        # 将置信度最高的 box 赋给临时变量
        tmpx1, tmpy1, tmpx2, tmpy2, ts = dets[i, 0], dets[i, 1], dets[i, 2], dets[i, 3], dets[i, 4]
        pos = i+1
        # NMS iterations, note that box_len changes if detection boxes fall below threshold
        while pos < box_len:
            x1, y1, x2, y2 = dets[pos, 0], dets[pos, 1], dets[pos, 2], dets[pos, 3]
            area = (x2 - x1 + 1)*(y2 - y1 + 1)
            iw = (min(tmpx2, x2) - max(tmpx1, x1) + 1)
            ih = (min(tmpy2, y2) - max(tmpy1, y1) + 1)
            if iw > 0 and ih > 0:
                overlaps = iw * ih
                ious = overlaps / ((tmpx2 - tmpx1 + 1) * (tmpy2 - tmpy1 + 1) + area - overlaps)
                if method == 1:    # 线性
                    if ious > Nt:
                        weight = 1 - ious
                    else:
                        weight = 1
                elif method == 2:  # gaussian
                    weight = np.exp(-(ious**2) / sigma)
                else:              # original NMS
                    if ious > Nt:
                        weight = 0
                    else:
                        weight = 1
                # 赋予该box新的置信度
                dets[pos, 4] = weight * dets[pos, 4]
                # 如果box得分低于阈值thresh，则通过与最后一个框交换来丢弃该框
                if dets[pos, 4] < threshold:
                    dets[pos, 0] = dets[box_len-1, 0]
                    dets[pos, 1] = dets[box_len-1, 1]
                    dets[pos, 2] = dets[box_len-1, 2]
                    dets[pos, 3] = dets[box_len-1, 3]
                    dets[pos, 4] = dets[box_len-1, 4]
                    box_len = box_len-1
                    pos = pos-1
            pos += 1
    keep = [i for i in range(box_len)]
    return keep
if __name__ == '__main__':
    dets = [[0, 0, 100, 101, 0.9], [5, 6, 90, 110, 0.7], [17, 19, 80, 120, 0.8], [10, 8, 115, 105, 0.5]]
    dets = np.array(dets)
    result = soft_nms(dets, 0.5)
    print(result)
```



# **5. 手写k-means**

```python
import pandas as pd
import numpy as np
import random as ran
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d # 
from sklearn.cluster import KMeans
 
def model_test():
    data = open_file("C:\\Users\\happy\\Desktop\\Iris1.csv")
    dataset = np.delete(data,-1,axis=1) #去掉最后一列
    k_means = KMeans(n_clusters=3) #构建模型
    k_means.fit(dataset)
    km4_labels = k_means.labels_
    ax = plt.subplot(projection='3d')
    ax.scatter(dataset[:,0],dataset[:,1],dataset[:,2],\
               c=km4_labels.astype(np.float))
    ax.set_zlabel('Z')  # 坐标轴
    ax.set_ylabel('Y')
    ax.set_xlabel('X')
    plt.show()
```



# **6.写python set的基本操作**

集合常用的两个场景是：1.去重（如：列表去重）；2.关系测试（如：取交集、取并集、取差集等）



# **7.写一个交叉熵损失函数**

交叉熵损失函数：实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。

![image-20220530201437208](https://s2.loli.net/2022/05/30/lAMbVQGDwE7JSrk.png)

```python
def cross_entropy(a,y):
    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))
#tensorflow版
loss = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y),reduction_indices=[1]))

#numpy版
loss = np.mean(-np.sum(y_*np.log(y),axis=1))
```



# **8. Softmax函数**

Softmax 函数：将激活值与所有神经元的输出值联系在一起，所有神经元的激活值加起来为1。第L层（最后一层）的第j个神经元的激活输出为：

![image-20220530201519954](https://s2.loli.net/2022/05/30/xQmeoJ1K82ZtuLP.png)

```python
def softmax(x):
    shift_x = x - np.max(x)#防止输入增大时输出为nan
    exp_x = np.exp(shift_x)
    return exp_x / np.sum(exp_x)
```



# **9.手推BN公式**

![image-20220530204021325](https://s2.loli.net/2022/05/30/9AzYmaBetjUfsyF.png)

上面的公式中m指的是mini-batch size。

```python
m = K.mean(X, axis=-1, keepdims=True)	#计算均值
std = K.std(X, axis=-1, keepdims=True)	#计算标准差
X_normed = (X - m)/(std + self.epsilon)	#归一化
out = self.gamma * X_normed + self.beta	#重构变换
```



# **10.Python打开一个文件，找出某个字符串最快的方法**

python 字符串查找有4个方法，1 find, 2 index方法，3 rfind方法,4 rindex方法。



# **11.写一下混淆矩阵、精确率和召回率的公式**



|          | 预测值=1 | 预测值=0 |
| :------: | :------: | :------: |
| 真实值=1 |    TP    |    FN    |
| 真实值=0 |    FP    |    TN    |

*准确度(Accuracy) = $\frac{T P+T N}{T P+F N+F P+T N}$*



*精度(precision, 或者PPV, positive predictive value) = $\frac{T P}{T P+F P}$*

*召回(recall, 或者敏感度，sensitivity，真阳性率，TPR，True Positive Rate) = $\frac{T P}{T P+F N}$*

*F1-值(F1-score) = $\frac{2 T P}{2 * T P+F N+FP}$*



# **12.要求画出LSTM的结构图**（写了cell state，hidden state，以及门结构和信号传递过程）

![image-20220530201745216](https://s2.loli.net/2022/05/30/kCwPEI2DulTQZKj.png)

### （1）忘记门：扔掉信息(细胞状态)

![å¨è¿éæå¥å¾çæè¿°](https://s2.loli.net/2022/05/30/C3l7tawIXYhfyH6.png)

> 第一步是决定从细胞状态里扔掉什么信息（也就是保留多少信息）。将上一步细胞状态中的信息选择性的遗忘 。
> 实现方式：通过sigmoid层实现的“忘记门”。以上一步的![h{_{t-1}}](https://s2.loli.net/2022/05/30/T52uagq4HcA6XSf.gif)和这一步的![x{_{t}}](https://s2.loli.net/2022/05/30/y4i32Z18AujVnWe.gif) 作为输入，然后为![C{_{t-1}}](https://s2.loli.net/2022/05/30/aFbkl2YC9VMSqdJ.gif)里的每个数字输出一个0-1间的值，记为![f_{t}](https://s2.loli.net/2022/05/30/6EroSXTbAdefvln.gif)，表示保留多少信息（1代表完全保留，0表示完全舍弃)
> 例子：让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的类别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。
> 例如，他今天有事，所以我… 当处理到‘’我‘’的时候选择性的忘记前面的’他’，或者说减小这个词对后面词的作用。

### （2）输入层门：存储信息(细胞状态)

![å¨è¿éæå¥å¾çæè¿°](https://s2.loli.net/2022/05/30/lwvIJycHuMexSGm.png)

> 第二步是决定在细胞状态里存什么。将新的信息选择性的记录到细胞状态中。 实现方式：包含两部分，
>     1.sigmoid层（输入门层）决定我们要更新什么值，这个概率表示为![i{_t}](https://s2.loli.net/2022/05/30/XPzRGQwcVECNIky.gif) 
>     2.tanh层创建一个候选值向量![\widetilde{C{_t}}](https://s2.loli.net/2022/05/30/G3XBxsegaLlMfpZ.gif)，将会被增加到细胞状态中。 我们将会在下一步把这两个结合起来更新细胞状态。
> 例子：在我们语言模型的例子中，我们希望增加新的主语的类别到细胞状态中，来替代旧的需要忘记的主语。 例如：他今天有事，所以我…当处理到‘’我‘’这个词的时候，就会把主语我更新到细胞中去。

### 更新细胞状态（细胞状态）

![在这里插入图片描述](https://s2.loli.net/2022/05/30/PXT6KCUjqDpenkt.png)

> **注意上面公式中的∗是对应元素乘，而不是矩阵的乘法**
>
> 更新旧的细胞状态
> 实现方式： ![f_{t}](https://s2.loli.net/2022/05/30/6EroSXTbAdefvln.gif)表示忘记上一次的信息![C{_{t-1}}](https://s2.loli.net/2022/05/30/aFbkl2YC9VMSqdJ.gif)的程度，![i{_t}](https://s2.loli.net/2022/05/30/XPzRGQwcVECNIky.gif)表示要将候选值![\widetilde{C{_t}}](https://s2.loli.net/2022/05/30/G3XBxsegaLlMfpZ.gif)加入的程度， 这一步我们真正实现了移除哪些旧的信息（比如上一句的主语），增加哪些新信息，最后得到了本细胞的状态![C{_t}](https://s2.loli.net/2022/05/30/Jbur4U8oEmlDT5s.gif) 。

### （3）输出层门：输出（隐藏状态）

![å¨è¿éæå¥å¾çæè¿°](https://s2.loli.net/2022/05/30/GfdKaeiwUnJb6Wh.png)

> 最后，我们要决定作出什么样的预测。 实现方式：
>
> 1. 我们通过sigmoid层（输出层门）来决定输出的本细胞状态![C{_t}](https://s2.loli.net/2022/05/30/Jbur4U8oEmlDT5s.gif) 的哪些部分；
> 2. 然后我们将细胞状态通过tanh层（使值在-1~1之间），然后与sigmoid层的输出相乘得到最终的输出![h{_t}](https://s2.loli.net/2022/05/30/7n3svATEZGoNRr1.gif) 。
>
> 所以我们只输出我们想输出的部分。 例子：在语言模型的例子中，因为它就看到了一个 代词，可能需要输出与一个 动词相关的信息。例如，可能输出是否代词是单数还是复数，这样如果是动词的话，我们也知道动词需要进行的词形变化。
> 例如：上面的例子，当处理到‘’我‘’这个词的时候，可以预测下一个词，是动词的可能性较大，而且是第一人称。 会把前面的信息保存到隐层中去。