---
layout:     post
title:      11.Training
subtitle:   深度学习
date:       2022-05-26
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Machine Learning 
    - DeepLearning 
---

# 在训练过程中loss出现NaN的原因以及可以采取的方法。

#### ==**1.梯度爆炸**==

**原因**：在学习过程中，梯度变得非常大，使得学习的过程偏离了正常的轨迹。



**症状**：观察输出日志(runtime log)中每次迭代的loss值，你会发现loss随着迭代有明显的增长，最后因为loss值太大以致于不能用浮点数去表示，所以变成了*NaN*。



**可采取方法：**

1. 降低学习率(至少一个数量级)
2. 若模型有多个loss层，不能降低基础的学习率，需要找到产生梯度爆炸的层，降低Training配置文件中该层的loss_weight.

==#### 2.错误的学习率策略及参数==

**原因**：在学习过程中，caffe不能得出一个正确的学习率，相反会得到*inf*或者*nan*的值。这些错误的学习率乘上所有的梯度使得所有参数变成无效的值。



**症状**：观察输出日志(runtime log)，可以看到学习率变成*NaN*



**可采取方法：**

(solver算是caffe的核心的核心，它协调着整个模型的运作。caffe程序运行必带的一个参数就是solver配置文件。)

修改*solver.prototxt*文件中所有能影响学习率的参数。比如，如果你设置的学习率策略是 *lr_policy: “poly”* ，而你又忘了设置最大迭代次数*max_iter*，那么最后你会得到*lr=NaN*

#### ==3.错误的损失函数==

**原因**：有时，在损失层计算损失值时会出现*NaN*的情况。比如，向*InfogainLoss*层没有归一化输入值，使用自定义的损失层等。



**症状**：观察输出日志(runtime log)的时候，你可能不会发现任何异常：loss逐渐下降，然后突然出现*NaN*。



**可采取的方法**：尝试重现该错误，打印损失层的值并调试。

举个栗子：有一回，我根据批量数据中标签出现的频率去归一化惩罚值并以此计算loss。如果有个label并没有在批量数据中出现，频率为0，结果loss出现了*NaN*的情况。在这种情况下，需要用足够大的batch来避免这个错误。

#### ==4.错误的输入==

**原因**：输入中存在NaN！



**症状**：一旦学习过程中碰到这种错误的输入，输出就会变成NaN。观察输出日志(runtime log)的时候，你可能也不会发现任何异常：loss逐渐下降，然后突然出现NaN。



**可采取的方法**：重建你的输入数据集(lmdb/leveldn/hdf5…)，确保你的训练集/验证集中没有**脏数据**（错误的图片文件）。调试时，使用一个简单的网络去读取输入，如果有一个输入有错误，这个网络的loss也会出现NaN。

#### ==5.Pooling层的步长大于核的尺寸==

由于一些原因，步长*stride*>核尺寸*kernel_size*的pooling层会出现*NaN*。比如：

```puppet
layer {
  name: "faulty_pooling"
  type: "Pooling"
  bottom: "x"
  top: "y"
  pooling_param {
    pool: AVE
    stride: 5
    kernel: 3
  }
}
```

结果y会出现*NaN*。

#### ==6.数值计算溢出==

这种类型的原因就比如softmax函数中需要计算exp(x)，当x过大时就会导致溢出，导致最终计算得到的结果是inf，最终的loss当然是NaN了。

**可采取的方法**：要确认你使用的softmax中在计算exp（x）做了相关处理（比如减去最大值等等）

#### ==7.training sample中出现了脏数据，或输入数据未进行归一化==

#### ==8.loss变成inf的原因可能是因为出现类似 log 0 的现象。当你的标签时1预测成0之后，在计算loss的时候就会去计算log 0.==

**解决方案：**你这个应该是一个分类问题。可以添加一个修剪操作，比如当label预测出来>0.999时全部设置为0.999，当label<0.001时，全部设置成为0.001。



#### 说法一：

说明训练不收敛了, 学习率太大，步子迈的太大导致梯度爆炸等都是有可能的，另外也有可能是网络的问题，网络结构设计的有问题。
我现在的采用方式是：

1. 弱化场景，将你的样本简化，各个学习率等参数采用典型配置，比如10万样本都是同一张复制的，让这个网络去拟合，如果有问题，则是网络的问题。否则则是各个参数的问题。

2. 如果是网络的问题，则通过不断加大样本的复杂度和调整网络（调整拟合能力）来改变。

3. 参数的微调，我个人感觉是在网络的拟合能力和样本的复杂度匹配的情况下，就是可以train到一定水平，然后想进行进一步优化的时候采用。

4. 参数的微调，楼上说得几个也算是一种思路吧，其他的靠自己去积累，另外将weights可视化也是一个细调起来可以用的方法，现在digits tf里面都有相关的工具.

  

![image-20220526160742641](https://s2.loli.net/2022/05/26/YV1hpRj3XgmAHrO.png)

**Gradient Clipping**(梯度剪裁)是一种在网络反向传播过程中，将误差导数改变或剪裁到阈值，并利用剪裁后的梯度来更新权值的方法。

通过重新缩放误差导数，权重的更新也将重新缩放，从而显著降低溢出或下溢的可能性。简单效果如图所示：

![image-20220526160800542](https://s2.loli.net/2022/05/26/McphnSNICLg15KY.png)

![image-20220526160849503](https://s2.loli.net/2022/05/26/25LXqszjkBfoDYM.png)

![image-20220526160914165](https://s2.loli.net/2022/05/26/9jiLfYyxmTlhwVC.png)