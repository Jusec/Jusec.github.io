---
layout:     post
title:      5.回归、SVM、K-Means
subtitle:   机器学习
date:       2022-05-01
author:     Mo
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Machine Learning   
---

# 回归

### **1.分类和回归的区别，各举例3个模型**

==定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。==



**常见分类模型**有感知机、朴素贝叶斯、逻辑回归(LR)、支持向量机(SVM)、神经网络等；



**常见回归模型**有线性回归、多项式回归、岭回归（L2正则化）、Lasso回归（L1正则化）等

### **2.线性回归和逻辑回归的区别**

**线性回归**：利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。一元线性回归分析：y=ax+by=ax+b，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示。多元线性回归分析：hθ(x)=θ0+θ1x1+...+θnxn，包括两个或两个以上的自变量，并且因变量和自变量是线性关系。



**逻辑回归**：逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。

逻辑回归的数学表达模型：

![image-20220501213336730](https://s2.loli.net/2022/05/01/Qx89JVYcInwp2GR.png)

**区别**：LR通常用于二分类，使用的是交叉熵损失函数；线性回归用于回归，使用的是均方误差损失函数

### **3.怎么优化LR？就是求解LR**

梯度下降、极大似然法。





# SVM--支持向量机

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。==SVM的学习策略就是间隔最大化==，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

### **1.SVM的原理**

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。

![image-20220502162425179](https://s2.loli.net/2022/05/02/by4Equ2OPdYUV3L.png)

即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

### **2. SVM的核函数了解哪些？为什么要用核函数？**

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

#### **①线性核函数**

![image-20220502162607521](https://s2.loli.net/2022/05/02/YTz1L3tJuxqFogf.png)

​	线性核，主要用于==线性可分==的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想。

#### **②多项式核函数**

![image-20220502162652973](https://s2.loli.net/2022/05/02/gvSD4Pdstm8Y7NJ.png)

​	多项式核函数可以实现==将低维的输入空间映射到高纬的特征空间==，但是多项式核函数的==参数多==，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。

#### **③高斯（RBF）核函数**

![image-20220502162824280](https://s2.loli.net/2022/05/02/2p9VCsg7FvT16bt.png)

​	高斯径向基函数是一种局部性强的核函数，其可以==将一个样本映射到一个更高维的空间内==，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数==参数要更少==，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。

#### **④sigmoid核函数**

![image-20220502162934462](https://s2.loli.net/2022/05/02/TUEDZnWuVlbtML2.png)



采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。



> **如何选择LR和SVM：**
>
> 如果特征的数量大到和样本数量差不多，则选用==LR或者线性核的SVM==；
>
> 
>
> 如果特征的数量小，样本的数量正常，则选用==SVM+高斯核函数==；
>
> 
>
> 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。

### **3.SVM如何解决线性不可分问题？**

间隔最大化，通过引入软间隔、核函数解决线性不可分问题。



软间隔：软间隔SVM允许部分点分布在间隔内部，此时可以解决硬间隔SVM的问题（只需将异常点放到间隔内部即可），因为间隔内部的点对于SVM的思想来说是一种错误，所以我们希望位于间隔内部的点尽可能少，其实是一种折中，即在错误较少的情况下获得不错的划分超平面

### **4. SVM优化的目标是啥？SVM的损失函数，SVM的适用场景**

SVM优化的目标是凸优化问题

![image-20220502163723005](https://s2.loli.net/2022/05/02/YOTdefk3I2jzUGa.png)

SVM的损失函数就是合页损失函数加上正则化项；

![image-20220502163733058](https://s2.loli.net/2022/05/02/C2yrmpT8GzucOQn.png)

### **SVM为什么要对偶(优化复杂度转变，核化)**

①首先是我们有不等式约束方程，这就需要我们写成min max的形式来得到最优解。而这种写成这种形式对x不能求导，所以我们需要转换成max min的形式，这时候，x就在里面了，这样就能对x求导了。而为了满足这种对偶变换成立，就需要满足KKT条件（KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）。



②对偶问题将原始问题中的约束转为了对偶问题中的等式约束



③方便核函数的引入



④改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。

### **6.==LR和SVM介绍+区别，什么场景用SVM比较好==**

> **相同点**：1. LR和SVM都是分类算法；
>
> ​	 ：2. 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。
>
> ​	 ：3. LR和SVM都是监督学习算法。
>
> ​	 ：4. LR和SVM都是判别模型。
>
> 
>
> **不同点：**1. 本质上是其损失函数(loss function)不同。注：LR的损失函数是 cross entropy loss(交叉熵损失函数)，AdaBoost的损失函数是expotional loss(指数损失函数) , SVM是hinge loss(合页损失)，常见的回归模型通常用均方误差 loss。
>
> ​	   2. 支持向量机只考虑==局部==的边界线附近的点，而逻辑回归考虑==全局==（远离的点对边界线的确定也起作用）。
>
> ​	   3. 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。
>
> ​	   4. 线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。
>
> ​	   5. SVM的损失函数就自带正则！而LR必须另外在损失函数上添加正则项。
>
> 
>
> SVM(支持向量机)主要用于分类问题,主要的应用场景有字符识别、面部识别、行人检测、文本分类等领域。

### **7.支持向量回归原理(SVR)**

SVR（支持向量回归）是SVM（支持向量机）中的一个重要的应用分支。SVR回归与SVM分类的区别在于，SVR的样本点最终只有一类，它所寻求的最优超平面不是SVM那样使两类或多类样本点分的“最开”，而是使所有的样本点离着超平面的总偏差最小





# K-Means(k均值)

K-Means是聚类算法中的最常用的一种，算法最大的特点是简单，好理解，运算速度快，但是只能应用于连续型的数据，并且一定要在聚类前需要手工指定要分成几类。

**算法思想**：

```
[选择K个点作为初始质心 

repeat 

    将每个点指派到最近的质心，形成K个簇 

    重新计算每个簇的质心 

until 簇不发生变化或达到最大迭代次数 ]
```

> **1.优点**
>
> - 原理简单容易实现
> - 聚类效果较优
> - 算法解释程度较强
> - 主要的调参的参数仅仅是簇数K
>
> **2.缺点**
>
> - 收敛较慢
> - 算法时间复杂度较高
> - 不能发现非凸形状的簇
> - 需要事先确定超参数K
> - 对噪声和离群点敏感
> - 结果不一定是全局最优，只能保证局部最优
> - 对数据类型要求较高，适合数值型数据

#### **1.K-means聚类的原理以及过程？**

K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。

#### **2.K-means聚类怎么衡量相似度的？**

==欧几里得距离和余弦相似度，==欧几里得距离会受指标不同单位刻度的影响，所以一般要先进行标准化，距离越大个体差异越大。余弦夹角相似度不受指标刻度的影响，余弦值落在-1到1之间，值越大，差异越小。当用户评分趋于一致时，但是评分值差距很大，余弦相似度倾向给出最优解。

![image-20220502175828201](https://s2.loli.net/2022/05/02/S4pTrNXMtZjbPhy.png)

#### **3.K值怎么来进行确定？**(轮廓系数法和手肘法)

（1）手肘法：

​	肘部法所使用的聚类评价指标为：数据集中所有样本点到其簇中心的距离之和的平方。但是肘部法选择的并不是误差平方和最小的![k](https://private.codecogs.com/gif.latex?k)，而是误差平方和突然变小时对应的![k](https://private.codecogs.com/gif.latex?k)值。

（2）轮廓系数法

​	轮廓系数是一种非常常用的聚类效果评价指标。该指标结合了内聚度和分离度两个因素。其具体计算过程如下：

假设已经通过[聚类算法](https://so.csdn.net/so/search?q=聚类算法&spm=1001.2101.3001.7020)将待分类的数据进行了聚类，并最终得到了![k](https://s2.loli.net/2022/05/02/2SB9nOdKCjbVJgc.gif)个簇。对于每个簇中的每个样本点![i](https://s2.loli.net/2022/05/02/otDBrWvZ3KSC4zL.gif)，分别计算其轮廓系数。具体地，需要对每个样本点![i](https://private.codecogs.com/gif.latex?i)计算以下两个指标：

![image-20220502171710797](https://s2.loli.net/2022/05/02/vbhfcHgBLTYPil2.png)

#### **4.简要阐述一下KNN算法和K-Means算法的区别**

①KNN算法是分类算法，分类算法肯定是需要有学习语料，然后通过学习语料的学习之后的模板来匹配我们的测试语料集，将测试语料集合进行按照预先学习的语料模板来分类；

②Kmeans算法是聚类算法，聚类算法与分类算法最大的区别是聚类算法没有学习语料集合。
